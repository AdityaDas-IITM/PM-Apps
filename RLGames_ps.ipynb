{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RLGames_ps.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.7.9 64-bit ('base': conda)"
    },
    "accelerator": "GPU",
    "interpreter": {
      "hash": "07efdcd4b820c98a756949507a4d29d7862823915ec7477944641bea022f4f62"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NEcxwh0Ej7T",
        "colab_type": "text"
      },
      "source": [
        "# `Hit the Gym: MiniProject for RL Games CPP`\n",
        "\n",
        "The exercises in this notebook are meant for CVI aspirants who wish to work on the RL Games CPP. \n",
        "\n",
        "**About the project:** The goal of the RL Games project is to explore and analyse current Reinforcement Learning Methods by deploying them on Atari games, Minesweeper, slither.io etc. Further, the techniques developed can be used to attack more complex problems such as Reconnaissance Blind Chess and Autonomous Driving.\n",
        "\n",
        "In this notebook, you'll use a popular Reinforcement Learning Technique called Q-learning to train an agent to play simple games using the python library OpenAI Gym. \n",
        "\n",
        "You may refer to the following while coding:\n",
        "\n",
        "Python reference: https://bit.ly/3ajUalZ and https://bit.ly/2UgiZKa\n",
        "\n",
        "OpenAI Gym Documentation: https://gym.openai.com/docs/\n",
        "\n",
        "\n",
        "### `RL Basics:`\n",
        "Reinforcement learning is an approach used in Machine Learning where the agent is allowed to interact with the system to learn its behaviour and come up with an optimal startegy to achieve an objective. The agent models the problem as a probabilistic state machine (a graph where the transition from one node to another has a probability distribution). Nodes in the model graph are called states and a transition from one state to another is called an action. Each state transition (which is a (state, action) pair) has a corresponding reward or penalty. The goal of the RL agent is to maximise the reward. \n",
        "\n",
        "Training an RL agent from scratch requires us to model the state space and the action space. In addition, we must also come up with suitable rewards for each state transition. The RL agent estimates this reward structure and executes actions so as to maximise them. The final performance of the RL agent is heavily dependent on how the system is modelled. Luckily for us, we **do not need to get into the mathematics** of Reinforcement Learning right now, thanks to the Python library Gym.\n",
        "\n",
        "\n",
        "Gym offers many in-built RL environments which you can use to play around with. These environments are Python classes with their state spaces, action spaces and rewards pre-defined. You will use two such environments (Taxi-v3 and FrozenLake8x8-v0) to train an agent accomplish a goal. You can find the documentation for these environments here:\n",
        "\n",
        "Taxi-v3 Documentation: https://gym.openai.com/envs/Taxi-v3/  \n",
        "\n",
        "FrozenLake Documentation: https://gym.openai.com/envs/FrozenLake8x8-v0/  \n",
        "\n",
        "To create a gym environment of 'Taxi-v3' you do this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMtvRD03J_JO",
        "colab_type": "code",
        "outputId": "b707f9f7-d204-4017-ef0f-296003ad4b2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        }
      },
      "source": [
        "import gym\n",
        "import numpy as np  \n",
        "# Create an environment of Taxi-v3:\n",
        "env = gym.make('Taxi-v3').env \n",
        "env.render()\n",
        "print(env.s)\n",
        "print(env.observation_space)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n|\u001b[34;1mR\u001b[0m: |\u001b[43m \u001b[0m: :\u001b[35mG\u001b[0m|\n| : | : : |\n| : : : : |\n| | : | : |\n|Y| : |B: |\n+---------+\n\n41\nDiscrete(500)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WauFKBJeKos0",
        "colab_type": "text"
      },
      "source": [
        "env.s is the current state of the environment.\n",
        "\n",
        "env.observation_space() returns the type and size of the observation or state space. Discrete implies that the states are discrete and not continuous. For games like Pacman, Gym uses another type of state space- Box- due to the large number of states. \n",
        "\n",
        "**Solving the Problem:** Now the goal of this game is to train the taxi to efficiently pick the passenger from the blue spot and drop them at the purple spot. The taxi can do the following actions: Move Up, Move Down, Move to the Left, Move to the Right, Pickup, Dropoff. The reward structure is such:\n",
        "1. -10 points for illegal dropoff/pickup actions\n",
        "2. +20 points if the passenger is dropped off at the correct location\n",
        "3. -1 point for every other action\n",
        "\n",
        "A paleolithic approach to this problem would be to pick an action at random and execute it. Eventually the passenger would get picked up and then dropped off at the correct location."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPwn4saqKiQN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "290949b9-51db-4ed3-a6b1-2958e7c26a69"
      },
      "source": [
        "state = env.reset()\n",
        "epochs = 0\n",
        "penalty, reward = 0, 0  # Penalty records the number of times the agent hits a wall\n",
        "frames = []\n",
        "done = False# Find out the role of 'done' and complete the statement for its initial condition \n",
        "while not done:\n",
        "    action = env.action_space.sample()\n",
        "    new_state, reward, done, info = env.step(action)\n",
        "    if new_state == state:\n",
        "      penalty+=1\n",
        "    state = new_state\n",
        "\n",
        "    frames.append({'frame': env.render(mode='ansi'), 'state': state, 'action': action, 'reward': reward})\n",
        "    epochs += 1\n",
        "print(\"Timesteps taken: {}\".format(epochs))\n",
        "print(\"Penalties incurred: {}\".format(penalty))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Timesteps taken: 1643\nPenalties incurred: 896\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Fwe9LT-8qn8",
        "colab_type": "text"
      },
      "source": [
        "Run the following cell to visualise the performance of the agent. It won't come as a surprise to see that the approach is quite bad. It is so because, the agent has no memory of the past and hence learns nothing. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgvVPJ9KQWJk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "94979ce1-b439-43f2-c482-55ef39ecf563"
      },
      "source": [
        "from IPython.display import clear_output\n",
        "from time import sleep\n",
        "def print_frames(frames):\n",
        "    for i, frame in enumerate(frames):\n",
        "        clear_output(wait = True)\n",
        "        print(frame['frame'])\n",
        "        print(f\"Timestep: {i+1}\")\n",
        "        print(f\"State: {frame['state']}\")\n",
        "        print(f\"Action: {frame['action']}\")\n",
        "        print(f\"Reward: {frame['reward']}\")\n",
        "        #sleep(0.1)\n",
        "print_frames(frames)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n|\u001b[35m\u001b[34;1m\u001b[43mR\u001b[0m\u001b[0m\u001b[0m: | : :G|\n| : | : : |\n| : : : : |\n| | : | : |\n|Y| : |B: |\n+---------+\n  (Dropoff)\n\nTimestep: 1643\nState: 0\nAction: 5\nReward: 20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKcVQ8OV8-Lv",
        "colab_type": "text"
      },
      "source": [
        "## `Enter Q-learning`\n",
        "\n",
        "A popular technique used in Reinforcement Learning is to have the agent maintain an estimate of the rewards that it would gain from executing a particular state transition (called the Q-table) which is updated after each step based on the reward obtained. The agent picks the action that yields the maximum reward at that particular state. That is, if a particular state transition ((state, action) pair) resulted in a good reward, the agent is better off repeating that action whenever that state is attained. \n",
        "\n",
        "**TASK 1:**\n",
        "\n",
        "Find out the update rule for Q-learning and implement Q-learning for the Taxi-v3 problem. Also, find out what 'exploration versus exploitation' is and use a suitable way to optimise on exploration and exploitation.\n",
        "\n",
        "You would need to set a few hyperparameters- learning rate ($\\alpha$), reward decay rate ($\\gamma$), number of episodes and exploration probability($\\epsilon$). Obtain the performance characteristics of the agent (that is, number of epochs per episode and average number of penalties per episode) for ($\\alpha$, $\\gamma$, episodes, $\\epsilon$) = (0.6, 0.9, 1000, 0.4)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ryw4gXzjQ2iO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "outputId": "7090c54b-8fb4-4a26-c0d0-a4cf4f7c4f0b"
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
        "(alpha, gamma, episodes, epsilon) = (0.6, 0.9, 1000, 0.4)\n",
        "total_epochs =0\n",
        "total_penalty = 0\n",
        "illegal_episode =[]\n",
        "for i in range(episodes):\n",
        "  state = env.reset()\n",
        "  epochs = 0\n",
        "  penalty, reward_tot = 0, 0  # Penalty records the number of times the agent hits a wall\n",
        "  illegal =0\n",
        "  \n",
        "  done = False# Find out the role of 'done' and complete the statement for its initial condition \n",
        "  while not done:\n",
        "    rand = random.uniform(0,1)\n",
        "    if rand < epsilon:\n",
        "      action = env.action_space.sample()\n",
        "    if rand >= epsilon:\n",
        "      action = np.argmax(q_table[state])\n",
        "    new_state, reward, done, info = env.step(action)\n",
        "    if new_state == state:\n",
        "      penalty+=1\n",
        "\n",
        "    if reward == -10:\n",
        "      illegal+=1\n",
        "\n",
        "    epochs += 1\n",
        "\n",
        "    oldq = q_table[state, action]\n",
        "    new_state_max = np.max(q_table[new_state])\n",
        "          \n",
        "    newq = (1 - alpha) * oldq + alpha * (reward + gamma * new_state_max)\n",
        "    q_table[state, action] = newq\n",
        "    state = new_state\n",
        "    epochs+=1\n",
        "  illegal_episode.append(illegal)\n",
        "  total_penalty+=penalty\n",
        "  total_epochs+=epochs\n",
        "\n",
        "total_penalty/=episodes\n",
        "total_epochs/=episodes\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sl1WvHdREsmv",
        "colab_type": "text"
      },
      "source": [
        "**TASK 2:**\n",
        "\n",
        "Now that you have trained the agent on Taxi-v3, try Q-learning on the FrozenLake8x8-v0 environment. After training, obtain the following performance characteristics- number of epochs per episode and average number of wins. What can you do to improve the performance of the agent?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGTlFXDrFyME",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "env = gym.make('FrozenLake8x8-v0').env \n",
        "q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
        "(alpha, gamma, episodes, epsilon) = (0.72, 0.5, 400, 1)\n",
        "total_epochs =0\n",
        "tot_wins =0\n",
        "max_epochs = 100\n",
        "check =[]\n",
        "for i in range(episodes):\n",
        "  state = env.reset()\n",
        "  epochs = 0\n",
        "  \n",
        "  done = False \n",
        "  while not done:\n",
        "    #wins = False\n",
        "    rand = random.uniform(0,1)\n",
        "    if rand < epsilon:\n",
        "      action = env.action_space.sample()\n",
        "    if rand >= epsilon:\n",
        "      action = np.argmax(q_table[state])\n",
        "    new_state, reward, done,__ = env.step(action)\n",
        "\n",
        "    if reward ==1: # when it reaches the goal\n",
        "      #wins=True\n",
        "      tot_wins+=1\n",
        "    elif reward==0 and done: # when it falls into a hole\n",
        "      reward=-10\n",
        "    else: # for any other movement\n",
        "      reward=-1\n",
        "    \n",
        "    \n",
        "\n",
        "    oldq = q_table[state, action]\n",
        "    new_state_max = np.max(q_table[new_state])\n",
        "          \n",
        "    newq = (1 - alpha) * oldq + alpha * (reward + gamma * new_state_max)\n",
        "    q_table[state, action] = newq\n",
        "    state = new_state\n",
        "    epochs+=1\n",
        "    \n",
        "\n",
        "  total_epochs+=epochs\n",
        "  epsilon*=0.3\n",
        "\n",
        "'''\n",
        "To improve performance let the loop run for more episodes and make the epsilon a decaying value so initially the model mostly explores and\n",
        "later the model mostly exploits. Also here I have changed the reward system so that the agent does not get stuck just moving back and forth and actually moves towards\n",
        "the goal which was not motivated enough in the previous system of rewards. This also improves the performance.\n",
        "From trial and error, the decay value of epsilon is kept to be very small for improved performance. A possible explanation is a lot of possible \n",
        "exploration paths are there but they dont lead to a goal any different. So it makes sense that if the agent finds a path that safely leads to the goal\n",
        "it should stick to it and not explore too much. Still some exploration could be favoured to find shorter paths. If no of episodes is to be increased, this\n",
        "value should also be changed.\n",
        "'''"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nTo improve performance let the loop run for more episodes and make the epsilon a decaying value so initially the model mostly explores and\\nlater the model mostly exploits. Also here I have changed the reward system so that the agent does not get stuck just moving back and forth and actually moves towards\\nthe goal which was not motivated enough in the previous system of rewards. This also improves the performance.\\nFrom trial and error, the decay value of epsilon is kept to be very small for improved performance. A possible explanation is a lot of possible \\nexploration paths are there but they dont lead to a goal any different. So it makes sense that if the agent finds a path that safely leads to the goal\\nit should stick to it and not explore too much. Still some exploration could be favoured to find shorter paths. If no of episodes is to be increased, this\\nvalue should also be changed.\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvuzOcw-v3mD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3f385729-e11e-4dcb-bb5c-bf6aa0df5eb6"
      },
      "source": [
        "print(f'won {tot_wins} out of {episodes} episodes') #0.3 decay"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "won 308 out of 400 episodes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LITRSgg9I33Z",
        "colab_type": "text"
      },
      "source": [
        "## `Further Motivation`\n",
        "\n",
        "In case you are curious about the mathematics of Reinforcement Learning, you check the following resources out:\n",
        "\n",
        "RL Lectures by Dr. David Silver: http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html\n",
        "\n",
        "Medium Blog Post on RL techniques: https://medium.com/@jonathan_hui/rl-introduction-to-deep-reinforcement-learning-35c25e04c199\n",
        "\n",
        "Deep Neural Networks (useful for Deep RL): http://cs231n.github.io/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjpYZfLKIOtJ",
        "colab_type": "text"
      },
      "source": [
        "If you are facing any issue or difficulty, you may PM (on WhatsApp):\n",
        "\n",
        "Arjun: 97392 19819"
      ]
    }
  ]
}